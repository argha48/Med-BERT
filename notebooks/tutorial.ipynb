{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for Med-BERT\n",
    "Step-by-step guide to the PyTorch implementation of [Med-BERT](https://www.nature.com/articles/s41746-021-00455-y) \n",
    "\n",
    "<sub><sup><em>\"Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction.\" NPJ digital medicine 4.1 (2021): 1-13., Rasmy, Laila, et al. <em><sub><sup>\n",
    "\n",
    "------------------------------------------\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The goal of Med-Bert is to obtain good representations of elctronic health records (EHR) to make predicitons for downstream tasks. <br>\n",
    "In order to do so we leverage the power of the pretraining fine-tuning paradigm using a transformer architecture.\n",
    "\n",
    "### Transformers\n",
    "\n",
    "Transformers $^{1}$, originally used for Natural Language Processing have proven their universality by showing SoTA results in fields like computer vision $^2$ and speech recognition $^3$. <br>\n",
    "Recently, the transformer architecture has also been applied to medical data and electronic health records in particular $^{4-6}$.<br> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "<sub><sup>[1] Vaswani, Ashish, et al. \"Attention is all you need.\" Advances in neural information processing systems 30 (2017).</sub></sup> <br>\n",
    "<sub><sup>[2] Dosovitskiy, Alexey, et al. \"An image is worth 16x16 words: Transformers for image recognition at scale.\" arXiv preprint arXiv:2010.11929 (2020).</sub></sup> <br>\n",
    "<sub><sup>[3] Polyak, Adam, et al. \"Speech resynthesis from discrete disentangled self-supervised representations.\" arXiv preprint arXiv:2104.00355 (2021).</sub></sup><br>\n",
    "<sub><sup>[4] Li, Yikuan, et al. \"BEHRT: transformer for electronic health records.\" Scientific reports 10.1 (2020): 1-12..</sub></sup><br>\n",
    "<sub><sup>[5] Shang, Junyuan, et al. \"Pre-training of graph augmented transformers for medication recommendation.\" arXiv preprint arXiv:1906.00346 (2019).</sub></sup><br>\n",
    "<sub><sup>[6] Pang, Chao, et al. \"CEHR-BERT: Incorporating temporal information from structured EHR data to improve prediction tasks.\" Machine Learning for Health. PMLR, 2021.</sub></sup><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
